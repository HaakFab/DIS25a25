{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Course â€“ Session 1\n",
    "\n",
    "Welcome to the first session of our NLP course! In this notebook, we'll cover:\n",
    "\n",
    "1. **Characters, Words, and Sentences**  \n",
    "2. **Text Normalization**  \n",
    "3. **Tokenization** (Whitespace, Standard, Subword, and Tweet Tokenizer)\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/fabian/miniconda3/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: tokenizers in /Users/fabian/miniconda3/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: click in /Users/fabian/miniconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/fabian/miniconda3/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/fabian/miniconda3/lib/python3.11/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from tokenizers) (0.23.5)\n",
      "Requirement already satisfied: filelock in /Users/fabian/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/fabian/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /Users/fabian/miniconda3/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /Users/fabian/miniconda3/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from transformers) (0.23.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/fabian/miniconda3/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fabian/miniconda3/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk tokenizers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fabian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/fabian/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/fabian/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/fabian/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Hello World! This is an example. Isn't it great to explore NLP?\n",
      "\n",
      "Length of the text (in characters): 63\n",
      "First character: H\n",
      "First 5 characters: Hello\n",
      "\n",
      "Naive split into words by whitespace:\n",
      "['Hello', 'World!', 'This', 'is', 'an', 'example.', \"Isn't\", 'it', 'great', 'to', 'explore', 'NLP?']\n",
      "\n",
      "Naive split into sentences by '.' :\n",
      "['Hello World! This is an example', \" Isn't it great to explore NLP?\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# SECTION 1: CHARACTERS, WORDS, SENTENCES\n",
    "# -------------------\n",
    "\n",
    "# Basic string operations to illustrate how Python handles text\n",
    "\n",
    "sample_text = \"Hello World! This is an example. Isn't it great to explore NLP?\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print()\n",
    "\n",
    "# 1. Length of the string (number of characters)\n",
    "print(\"Length of the text (in characters):\", len(sample_text))\n",
    "\n",
    "# 2. Accessing individual characters\n",
    "print(\"First character:\", sample_text[0])\n",
    "print(\"First 5 characters:\", sample_text[:5])\n",
    "print()\n",
    "\n",
    "# 3. Splitting into words naively (by whitespace)\n",
    "words_naive = sample_text.split()\n",
    "print(\"Naive split into words by whitespace:\")\n",
    "print(words_naive)\n",
    "print()\n",
    "\n",
    "# 4. Splitting into sentences (very naively by '.')\n",
    "sentences_naive = sample_text.split('.')\n",
    "print(\"Naive split into sentences by '.' :\")\n",
    "print(sentences_naive)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer vs. WordNet Lemmatizer:\n",
      "\n",
      "Word: studies    | Stem: studi      | Lemma: study     \n",
      "Word: studying   | Stem: studi      | Lemma: studying  \n",
      "Word: studied    | Stem: studi      | Lemma: studied   \n",
      "Word: leaves     | Stem: leav       | Lemma: leaf      \n",
      "Word: leaving    | Stem: leav       | Lemma: leaving   \n",
      "Word: better     | Stem: better     | Lemma: better    \n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# SECTION 2: TEXT NORMALIZATION\n",
    "# -------------------\n",
    "# We'll demonstrate the use of Porter Stemmer and WordNet Lemmatizer from NLTK.\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Make sure you have the relevant NLTK data downloaded:\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "example_words = [\"studies\", \"studying\", \"studied\", \"leaves\", \"leaving\", \"better\"]\n",
    "\n",
    "print(\"Porter Stemmer vs. WordNet Lemmatizer:\\n\")\n",
    "for word in example_words:\n",
    "    stem = porter.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word)  # default is noun lemmatization\n",
    "    print(f\"Word: {word:<10} | Stem: {stem:<10} | Lemma: {lemma:<10}\")\n",
    "\n",
    "# Note that the lemmatizer may need part-of-speech tags for full accuracy. \n",
    "# For example, 'studying' can lemmatize differently as a verb vs. a noun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text for tokenization:\n",
      "I'm soooo excited!!! #NLP is awesome. Check this out: https://example.com ðŸ˜€ðŸ”¥ #fun @user\n",
      "\n",
      "1) Whitespace splitting:\n",
      "[\"I'm\", 'soooo', 'excited!!!', '#NLP', 'is', 'awesome.', 'Check', 'this', 'out:', 'https://example.com', 'ðŸ˜€ðŸ”¥', '#fun', '@user']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# SECTION 3: TOKENIZATION\n",
    "# -------------------\n",
    "# We'll look at several different approaches:\n",
    "# 1. Simple whitespace splitting\n",
    "# 2. Standard tokenizer (nltk.word_tokenize)\n",
    "# 3. Subword tokenization (Byte-Pair Encoding using Hugging Face)\n",
    "# 4. Twitter tokenizer (nltk.TweetTokenizer)\n",
    "\n",
    "# We'll create a sample text including emojis, hashtags, etc.\n",
    "\n",
    "text_for_tokenization = (\n",
    "    \"I'm soooo excited!!! #NLP is awesome. Check this out: https://example.com \"\n",
    "    \"ðŸ˜€ðŸ”¥ #fun @user\"\n",
    ")\n",
    "\n",
    "print(\"Sample text for tokenization:\")\n",
    "print(text_for_tokenization)\n",
    "print()\n",
    "\n",
    "# --- 3.1 Simple whitespace splitting ---\n",
    "whitespace_tokens = text_for_tokenization.split()\n",
    "print(\"1) Whitespace splitting:\")\n",
    "print(whitespace_tokens)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) NLTK's Standard Tokenizer (word_tokenize):\n",
      "['I', \"'m\", 'soooo', 'excited', '!', '!', '!', '#', 'NLP', 'is', 'awesome', '.', 'Check', 'this', 'out', ':', 'https', ':', '//example.com', 'ðŸ˜€ðŸ”¥', '#', 'fun', '@', 'user']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3.2 Standard Tokenizer (NLTK) ---\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokens = word_tokenize(text_for_tokenization)\n",
    "print(\"2) NLTK's Standard Tokenizer (word_tokenize):\")\n",
    "print(nltk_tokens)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) Subword Tokenization (BERT):\n",
      "['i', \"'\", 'm', 'soo', '##oo', 'excited', '!', '!', '!', '#', 'nl', '##p', 'is', 'awesome', '.', 'check', 'this', 'out', ':', 'https', ':', '/', '/', 'example', '.', 'com', '[UNK]', '#', 'fun', '@', 'user']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3.3 Subword Tokenization ---\n",
    "# Requires: pip install transformers\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "subword_tokens = bert_tokenizer.tokenize(text_for_tokenization)\n",
    "\n",
    "print(\"3) Subword Tokenization (BERT):\")\n",
    "print(subword_tokens)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4) NLTK's Twitter Tokenizer (TweetTokenizer):\n",
      "[\"I'm\", 'soooo', 'excited', '!', '!', '!', '#NLP', 'is', 'awesome', '.', 'Check', 'this', 'out', ':', 'https://example.com', 'ðŸ˜€', 'ðŸ”¥', '#fun', '@user']\n"
     ]
    }
   ],
   "source": [
    "# --- 3.4 Twitter Tokenizer (NLTK) ---\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(text_for_tokenization)\n",
    "\n",
    "print(\"4) NLTK's Twitter Tokenizer (TweetTokenizer):\")\n",
    "print(tweet_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Comparing the Different Tokenization Outputs\n",
    "\n",
    "1. **Whitespace** â€“ Splits only on spaces (may miss punctuation, emojis, etc.).  \n",
    "2. **NLTK Standard** â€“ Splits punctuation, contractions more carefully.  \n",
    "3. **Subword ** â€“ Splits into subword units (useful for handling unknown words, morphological variations).  \n",
    "4. **Tweet Tokenizer** â€“ Specifically designed for social media text, handling hashtags, mentions, emojis, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Make sure you have these downloaded if you haven't already:\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Sample text with hashtags, emojis, URL, etc.\n",
    "text_for_tokenization = (\n",
    "    \"I'm soooo excited!!! :-) #NLP is awesome. Check this out: https://example.com \"\n",
    "    \"ðŸ˜€ðŸ”¥ #fun @user\"\n",
    ")\n",
    "\n",
    "# 1) Simple Whitespace Split\n",
    "\n",
    "# 2) NLTK Standard Tokenizer\n",
    "\n",
    "# 3) Subword Tokenizer (BertTokenizer)\n",
    "\n",
    "# 4) NLTK Tweet Tokenizer\n",
    "\n",
    "\n",
    "# compare the tokenized texts: What is different?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Byte Pair Encoding\n",
    "To better understand what is happening in BPE, let's implement PPE ourselves.\n",
    "\n",
    "1. Complete the function byte_pair_encoding.\n",
    "2. Show the vocabulary of the encoded result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    }
   ],
   "source": [
    "# Tip: You can use the following code to find the most frequent pair in a dictionary:\n",
    "sample_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "print(max(sample_dict, key=sample_dict.get))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# defaultdict automatically counts the number of occurrences of each element returning a dictionary\n",
    "\n",
    "def get_pair_stats(tokenized_text):\n",
    "    \"\"\"\n",
    "    Given a tokenized text (list of tokens), return a dictionary\n",
    "    mapping each adjacent pair to its frequency (count of occurrences).\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for tokens in tokenized_text:\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i+1])\n",
    "            pairs[pair] += 1\n",
    "    return pairs\n",
    "\n",
    "def merge_tokens(pair_to_merge, tokenized_text):\n",
    "    \"\"\"\n",
    "    Merge all occurrences of 'pair_to_merge' in the tokenized_text.\n",
    "    \n",
    "    pair_to_merge: a tuple of two tokens to be merged.\n",
    "    tokenized_text: list of lists of tokens.\n",
    "    \"\"\"\n",
    "    new_token = \"\".join(pair_to_merge)\n",
    "    merged_text = []\n",
    "    for tokens in tokenized_text:\n",
    "        merged_tokens = []\n",
    "        skip_next = False\n",
    "        for i in range(len(tokens)):\n",
    "            if skip_next:\n",
    "                skip_next = False\n",
    "                continue\n",
    "            \n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair_to_merge:\n",
    "                # Merge the two tokens\n",
    "                merged_tokens.append(new_token)\n",
    "                skip_next = True\n",
    "            else:\n",
    "                merged_tokens.append(tokens[i])\n",
    "        merged_text.append(merged_tokens)\n",
    "    return merged_text\n",
    "\n",
    "\n",
    "\n",
    "def byte_pair_encoding(texts, max_merges=10):\n",
    "    \"\"\"\n",
    "    Performs Byte Pair Encoding (BPE) on a list of text strings.\n",
    "    \n",
    "    1. Tokenize each text string initially by splitting into characters\n",
    "       (you might adjust this to split by subwords, add special symbols, etc.).\n",
    "    2. Iteratively find and merge the most frequent pair up to max_merges times.\n",
    "    \n",
    "    :param texts: List of raw text strings to be encoded.\n",
    "    :param max_merges: The maximum number of pair merges to perform (stopping criterion).\n",
    "\n",
    "    :return: (encoded_texts, merges)\n",
    "       - encoded_texts: Final tokenized text after BPE merges.\n",
    "       - merges: List of merges performed (in order).\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_text = []\n",
    "    merges = []\n",
    "\n",
    "    # 1. Tokenize the text into characters\n",
    "    for text in texts:\n",
    "        # Convert to list of characters; you could also treat whitespaces or special tokens differently\n",
    "        # E.g. text: \"hello world\" -> ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. Find the most frequent pair in the entire corpus, for max_merges iterations\n",
    "     \n",
    "        # Sort pairs by frequency (highest first)\n",
    "             \n",
    "        \n",
    "        # 3. Merge that pair throughout the tokenized text\n",
    "        \n",
    "        \n",
    "    return tokenized_text, merges\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sample_texts = [\n",
    "    \"Wild wild west is a movie\",\n",
    "    \"Kanye West is a rapper\",\n",
    "    \"A good movieis a good movie\",\n",
    "    \"A Wrapper is a person who wraps gifts\",\n",
    "    \"A Burrito is a Mexican wrap per se\",\n",
    "    \"Per perdes means by foot\"\n",
    "]\n",
    "\n",
    "# Perform BPE with up to 10 merges\n",
    "encoded_result, performed_merges = byte_pair_encoding(sample_texts, max_merges=10)\n",
    "\n",
    "print(\"Final Tokenized Text:\")\n",
    "for line in encoded_result:\n",
    "    print(line)\n",
    "\n",
    "print(\"\\nMerges Performed:\")\n",
    "print(performed_merges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
